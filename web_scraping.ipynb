{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b7e3d2-d622-4adf-86a1-66171ecdf301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages. If unable to import, try \"!pip install package\" first.\n",
    "# Selenium package is required. This package has to be used with Google Chrome Driver.\n",
    "# Here is a tutorial: https://blog.csdn.net/qq_48736958/article/details/115179198\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "import os\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import selenium.webdriver.support.ui as ui\n",
    "import pandas as pd\n",
    "import sys,time\n",
    "from tqdm import tqdm\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5088ab-b5d9-4378-b146-b399c1bd5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several functions to be used later\n",
    "def clean_Table_of_Contents(lst):\n",
    "    clean_lst = []\n",
    "    for item in lst:\n",
    "        # Remove weird symbols like \\u2002\n",
    "        clean_item = re.sub(r'[\\u2000-\\u200a]', '', item)\n",
    "        clean_item = clean_item.strip()\n",
    "        clean_item = re.sub(r'(\\s|^)\\d+$', '', clean_item)\n",
    "        clean_lst.append(clean_item.strip())\n",
    "    \n",
    "    clean_lst = [item for item in clean_lst if item != \"\"]\n",
    "    return clean_lst\n",
    "\n",
    "def find_element_index(lst, target_word):\n",
    "    for i, item in enumerate(lst):\n",
    "        if target_word in item.lower():\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392d2eb4-13c4-4fe9-b39d-a395ba00c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_list = [\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/701288/000110465924044922/tm242747d4_def14a.htm\",\n",
    "\"https://www.sec.gov/Archives/edgar/data/775368/000120677421000710/y3856321-def14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1475922/000119312524078288/d775324ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/6201/000119312524114008/d636721ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/3197/000155837024004904/ceco-20240520xdef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/764622/000076462224000033/pnw-20240404.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/2178/000000217824000043/ae-20240327.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/766421/000095017024038178/alk-20240328.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/701288/000110465924044922/tm242747d4_def14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/3453/000155837024002901/matx-20240425xdef14a.htm\",\n",
    "\"https://www.sec.gov/Archives/edgar/data/775368/000120677421000710/y3856321-def14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/773840/000077384024000032/hon-20240402.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/4447/000119312524088446/d520445ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1475922/000119312524078288/d775324ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/4904/000000490424000023/aep-20240313.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/4962/000119312524068837/d558747ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/5272/000000527224000034/aig-20240402.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/5981/000095017024048460/avd-20240425.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1037868/000103786824000022/ame-20240312.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/318154/000119312524099062/d445034ddef14a.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/821026/000082102624000108/ande-20240313.htm\",\n",
    "\"https://www.sec.gov/Archives/edgar/data/7039/000000703921000041/trecora-2021proxystatement.htm\",\n",
    "\"https://www.sec.gov/ix?doc=/Archives/edgar/data/7084/000119312524091968/d540412ddef14a.htm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb43f5a-a8fb-4120-9dc5-5b64f727566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Chrome Driver.\n",
    "options = Options()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--disable-blink-features\")\n",
    "##options.add_argument(\"--proxy-server=https://pr.oxylabs.io:7777\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "})\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4d4f09-6a5e-4708-96d8-378c22644017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5/23 [00:21<01:19,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7/23 [00:30<01:07,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Content Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 10/23 [00:43<00:58,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Content Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 15/23 [01:03<00:32,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 18/23 [01:16<00:20,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Content Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 21/23 [01:27<00:07,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 22/23 [01:29<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:36<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "Outputs = []\n",
    "for Proxy_statement_url in tqdm(URL_list):\n",
    "    ## First of all, we need to reconstrurct the url so that the web page could be scraped\n",
    "    Proxy_statement_url_revised = Proxy_statement_url.replace(\"ix?doc=/\",\"\")\n",
    "    driver.get(Proxy_statement_url_revised)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Secondly, we need to locate the Table of Content and obtain all headlines.\n",
    "\n",
    "    # Obtain all possible tables in the file\n",
    "    # Locate the one with keywords like \"pay ratio\"\n",
    "    # The table with our keywords is most likely to be \"Table of Content\"\n",
    "    list_of_tables = WebDriverWait(driver, 2).until(EC.presence_of_all_elements_located((By.XPATH, \".//table\")))\n",
    "    for table in list_of_tables:\n",
    "        table_of_content = table.text.lower()\n",
    "        if \"pay ratio\" in table_of_content:            \n",
    "            break\n",
    "    \n",
    "    if \"pay ratio\" not in table_of_content:\n",
    "        Outputs.append({\"URL\":Proxy_statement_url,\"Pay_Ratio_Paragraph\":\"Need Manual Work\",\"Reasons\":\"Table of Content Not Found\"})\n",
    "        print(\"Table of Content Not Found\")\n",
    "        continue\n",
    "            \n",
    "    TOC_list = clean_Table_of_Contents(table_of_content.split(\"\\n\"))\n",
    "\n",
    "    # Obtain our target headline and the headline next to our target headline within \"Table of Content\"\n",
    "    Target_headline_index = find_element_index(TOC_list, \"pay ratio\")\n",
    "    Next_headline_index = Target_headline_index + 1\n",
    "    Target_headline = TOC_list[Target_headline_index]\n",
    "    Next_headline = TOC_list[Next_headline_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Thirdly, Obtain the full text of the file. And Extract the Paragraph of \"Pay Ratio\"\n",
    "\n",
    "    # Obtain the full text of the file\n",
    "    Full_text = driver.find_element(By.XPATH, \"//body\").text.lower()\n",
    "\n",
    "    # Obtain the text of \"Table of Content\"\n",
    "    TOC_text = table_of_content\n",
    "\n",
    "    # Remove the text of \"Table of Content\" from the full text. Convert all paragraphs into list.\n",
    "    Text_of_File = Full_text.replace(TOC_text,\"\").split(\"\\n\")\n",
    "\n",
    "    # Remove space at the front and at the end of each paragraph. (This can facilitate our matching later)\n",
    "    Text_of_File = [item.strip().lower() for item in Text_of_File]\n",
    "\n",
    "    # Locate the position of the two headlines within the paragraphs and extract the text\n",
    "    try:\n",
    "        Target_headline_index_2 = Text_of_File.index(Target_headline.lower())\n",
    "        Next_headline_index_2 = Text_of_File[Target_headline_index_2:].index(Next_headline.lower())\n",
    "        Pay_Ratio_Paragraph = \"\\n\".join(Text_of_File[Target_headline_index_2:Target_headline_index_2 + Next_headline_index_2])\n",
    "    except:\n",
    "        Outputs.append({\"URL\":Proxy_statement_url,\"Pay_Ratio_Paragraph\":\"Need Manual Work\",\"Reasons\":\"Headline Not Found\"})\n",
    "        print(\"Headline Not Found\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    ## Finally, Organize the results and append them in a list\n",
    "    Outputs.append({\"URL\":Proxy_statement_url,\"Pay_Ratio_Paragraph\":Pay_Ratio_Paragraph})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2122e6db-9487-4bd6-8523-b4a3e820518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Outputs).to_excel(r\"C:\\Users\\admin\\Downloads\\Testing Outputs.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47111487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL  \\\n",
      "0  https://www.sec.gov/ix?doc=/Archives/edgar/dat...   \n",
      "1  https://www.sec.gov/Archives/edgar/data/775368...   \n",
      "2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...   \n",
      "3  https://www.sec.gov/ix?doc=/Archives/edgar/dat...   \n",
      "4  https://www.sec.gov/ix?doc=/Archives/edgar/dat...   \n",
      "\n",
      "                                 Pay_Ratio_Paragraph             Reasons  \n",
      "0  pay ratio disclosure\\npursuant to item 402(u) ...                 NaN  \n",
      "1  ceo pay ratio\\ngeneral\\nwe are providing the f...                 NaN  \n",
      "2  pay ratio\\nin august 2015, pursuant to a manda...                 NaN  \n",
      "3  ceo pay ratio\\nas required by section 953(b) o...                 NaN  \n",
      "4                                   Need Manual Work  Headline Not Found  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(Outputs).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
